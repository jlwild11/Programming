{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date, timedelta, datetime\n",
    "import os,sys,inspect\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import shutil\n",
    "\n",
    "start_time = datetime.now().time()\n",
    "\n",
    "Export_Day_Num = 4 \n",
    "# Number of days the PI Error Report is oging back.  This variable is manual and only used in file names.  May be updated later by calculating the date range of the data later.\n",
    "\n",
    "Sheet_Name = \"Circuits_Last_\"+str(Export_Day_Num)+\"_Days\"\n",
    "# Sheet name of the Excel Files.\n",
    "\n",
    "today = datetime.now() \n",
    "date = today.strftime(\"%Y%m%d\")\n",
    "#current date YYYY/MM/DD format\n",
    "\n",
    "today_and_time = datetime.now() \n",
    "date_time = today_and_time.strftime(\"%Y%m%d_%H%M\")\n",
    "date_time_format = today_and_time.strftime(\"%Y/%m/%d %H:%M\")\n",
    "#used to create time specifc folders for multiple data exports in the same day.\n",
    "\n",
    "home_dir = os.path.dirname(os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe()))))\n",
    "#the home folder for PI_Error_Report\n",
    "\n",
    "parent_of_home_dir = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))))\n",
    "#the parent folder of the home folder for PI_Error_Report.  Will be used to access the Roster Table which will be used for other Reports.\n",
    "\n",
    "CSV_Staging = home_dir + \"\\\\\" + \"Excel_Data_Exports\" + \"\\\\\" + \"CSV_Staging\"\n",
    "#folder location to place all data exports to create the PI Error Report with\n",
    "\n",
    "Excel_Data_Export_Location = home_dir + \"\\\\\" + \"Excel_Data_Exports\" + \"\\\\\" + date_time\n",
    "#folder created with the current date and time where all data will be moved to and PI Error Report will be created.\n",
    "\n",
    "filenames = []\n",
    "#used to hold filenames in CSV Staging\n",
    "\n",
    "filenames_csv_path_df = [] \n",
    "#used to hold the temporary pandas dataframe names that is associated with each filename in CSV Staging\n",
    "\n",
    "out_Circuits_Combined = \"Circuits_Combined\"\n",
    "#Location of Excel file created when all CSV data exports are combined.\n",
    "\n",
    "circuits_xlsx_path = Excel_Data_Export_Location + \"\\\\\" + out_Circuits_Combined + \"\\\\\" +  \"EVM_\" + date + \"_PI_Error_Report_Data_Export_Circuits.xlsx\"\n",
    "#Name of excel file created of the cobined CSV files\n",
    "\n",
    "pi_error_report_dir = \"PI_Error_Report\"\n",
    "#Folder for the PI Error Report being created with the current data exported\n",
    "\n",
    "pi_error_report_xlsx_path = Excel_Data_Export_Location + \"\\\\\" + pi_error_report_dir + \"\\\\\" +  \"EVM_\" + date + \"_PI_Error_Report.xlsx\" #Filename of the PI Error Report created with all necessary columns, formatting, and tables to present and submit.\n",
    "\n",
    "roster_xlsx_path = home_dir + \"\\\\\" + \"Roster_For_Reports\" + \"\\\\\" + \"Roster_For_Reports.xlsx\" \n",
    "#location of roster of PIs.  Migh create a section to copy from a parent folder location so the same roster can be used in other reports.\n",
    "\n",
    "# colors for print commands\n",
    "PURPLE = '\\033[95m'\n",
    "CYAN = '\\033[96m'\n",
    "DARKCYAN = '\\033[36m'\n",
    "BLUE = '\\033[94m'\n",
    "GREEN = '\\033[92m'\n",
    "YELLOW = '\\033[93m'\n",
    "RED = '\\033[91m'\n",
    "BOLD = '\\033[1m'\n",
    "UNDERLINE = '\\033[4m'\n",
    "NORMAL = '\\033[0m'\n",
    "\n",
    "#set otion to display all columns when using \"dataframe\".head()\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "print(BOLD + BLUE + \"Home Directory: \" + NORMAL + home_dir)\n",
    "print(BOLD + BLUE + \"Today's Date:\" + NORMAL, date)\n",
    "print(BOLD + BLUE + \"Today's Date and Time:\" + NORMAL, date_time)\n",
    "print(BOLD + BLUE + \"Start Time:\" + NORMAL, start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#move files in CSV Staging into Excel Data export location folder\n",
    "source  = CSV_Staging\n",
    "destination  = Excel_Data_Export_Location\n",
    "shutil.move(source, destination)\n",
    "\n",
    "#create new CSV Staging folder after previous one has been removed\n",
    "if not os.path.exists(CSV_Staging):\n",
    "    os.mkdir(CSV_Staging)\n",
    "#Create a folder for the excel file created by the combined csv files\n",
    "if not os.path.exists(Excel_Data_Export_Location + \"\\\\\" + out_Circuits_Combined):\n",
    "    os.mkdir(Excel_Data_Export_Location + \"\\\\\" + out_Circuits_Combined)\n",
    "#create a folder where the excel file for the PI Error Report will be located\n",
    "if not os.path.exists(Excel_Data_Export_Location + \"\\\\\" + pi_error_report_dir):\n",
    "    os.mkdir(Excel_Data_Export_Location + \"\\\\\" + pi_error_report_dir)\n",
    "\n",
    "#add all csv files in CSV Staging to filenames variable\n",
    "filenames = glob.glob(Excel_Data_Export_Location + '/*.csv')\n",
    "\n",
    "print(BOLD + BLUE + \"Filenames:\" + NORMAL)\n",
    "for x in filenames:\n",
    "    print(x + \"\\n\")\n",
    "\n",
    "#create the list of data frame varaible names to be assigned a filename\n",
    "i = 0\n",
    "filenames_csv_path_df = []\n",
    "while i < len(filenames):\n",
    "    filenames_csv_path_df.append(\"df_\"+str(i))\n",
    "    i=i+1\n",
    "\n",
    "#print the data frame variables created with each associated filename\n",
    "print(BOLD + GREEN + \"Dataframe name and Filename\"+ NORMAL)\n",
    "i = 0\n",
    "while i < len(filenames):\n",
    "    print(filenames_csv_path_df[i] + \" = \" + filenames[i] )\n",
    "    print('\\n')\n",
    "    i=i+1\n",
    "\n",
    "#associate a filename with each data frame variable name created  \n",
    "i = 0\n",
    "while i < len(filenames):\n",
    "    filenames_csv_path_df[i] = pd.read_csv(filenames[i])\n",
    "    i=i+1\n",
    "\n",
    "#combine all csv files in filenames using the list variable all the data frame varibles were created in\n",
    "Circuits_df = pd.concat(filenames_csv_path_df, axis=0)\n",
    "#write the combined csv variable to an excel file\n",
    "Circuits_df.to_excel(circuits_xlsx_path,sheet_name=Sheet_Name, index=False)\n",
    "#read the excel file and assign it to a variable\n",
    "Circuits = pd.read_excel(circuits_xlsx_path)\n",
    "#view the first row of the excel file\n",
    "Circuits_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attach Roster and filter to only have rows where the EVM_LAST_EDITOR is in the Roster table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Circuits = pd.read_excel(circuits_xlsx_path)\n",
    "Roster = pd.read_excel(roster_xlsx_path)\n",
    "cols_to_keep = ['Username_awrr', 'Last', 'First','Full_Name','Co','Company','Email','Include_awrr_in_PIER']\n",
    "Roster = Roster[ cols_to_keep]\n",
    "\n",
    "pi_error_report_temp = pd.merge(left=Circuits,right=Roster, left_on='EVM_LAST_EDITOR', right_on = 'Username_awrr', how='inner')\n",
    "PI_Error_Report = pi_error_report_temp.to_excel(pi_error_report_xlsx_path,sheet_name=Sheet_Name, index = False)\n",
    "PI_Error_Report = pd.read_excel(pi_error_report_xlsx_path)\n",
    "PI_Error_Report.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Helper Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Total Veg Points Created - equals 1 for all rows\n",
    "\n",
    "\n",
    "#Diameter R-Scale\n",
    "\n",
    "\n",
    "#Prescription R-Scale\n",
    "\n",
    "\n",
    "#Week(M-Su)\n",
    "\n",
    "\n",
    "#EVM_LAST_EDITED_PST\n",
    "\n",
    "\n",
    "#EVM_LAST_EDITED_DAY\n",
    "\n",
    "\n",
    "#Created_PST\n",
    "\n",
    "\n",
    "#Created_Day\n",
    "\n",
    "\n",
    "#New_or_Old_Veg\n",
    "\n",
    "\n",
    "#Comments_Contain_Delete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Previous Prescription Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Previous Prescription\n",
    "\n",
    "\n",
    "#Prescription_Changed\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prescription and Status do not match\n",
    "\n",
    "#Missing_Species\n",
    "PI_Error_Report.loc[(PI_Error_Report['SPECIES'].notnull()), 'Missing_Species'] = 0\n",
    "PI_Error_Report.loc[(PI_Error_Report['SPECIES'].isnull()), 'Missing_Species'] = 1\n",
    "\n",
    "#Missing_Diameter\n",
    "PI_Error_Report.loc[(PI_Error_Report['DIAMETER'].notnull()), 'Missing_Diameter'] = 0\n",
    "PI_Error_Report.loc[(PI_Error_Report['DIAMETER'].isnull()), 'Missing_Diameter'] = 1\n",
    "\n",
    "#Missing Height\n",
    "PI_Error_Report.loc[(PI_Error_Report['HEIGHT'].notnull()), 'Missing_Height'] = 0\n",
    "PI_Error_Report.loc[(PI_Error_Report['HEIGHT'].isnull()), 'Missing_Height'] = 1\n",
    "\n",
    "#Missing TAT Result\n",
    "PI_Error_Report.loc[(PI_Error_Report['TAT_RESULT'].notnull()), 'Missing_TAT_Result'] = 0\n",
    "PI_Error_Report.loc[(PI_Error_Report['TAT_RESULT'].isnull()), 'Missing_TAT_Result'] = 1\n",
    "\n",
    "#TAT Result and Prescription fo not match\n",
    "\n",
    "\n",
    "#Prescription and Diameter R-Scales do not match\n",
    "\n",
    "\n",
    "#Brush Prescription should have WM Work Type be Not Needed\n",
    "\n",
    "\n",
    "#Prescription Edited and TAT is Blank\n",
    "\n",
    "\n",
    "\n",
    "#TEST COLUMN -  DELETE\n",
    "PI_Error_Report.loc[(PI_Error_Report['WORK_VERIFICATION_STATUS'].notnull()), 'Missing_WV_Status'] = 0\n",
    "PI_Error_Report.loc[(PI_Error_Report['WORK_VERIFICATION_STATUS'].isnull()), 'Missing_WV_Status'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sum of Errors\n",
    "\n",
    "\n",
    "#Veg Point Has Errors\n",
    "\n",
    "\n",
    "#Include_Veg_Point_In_PIER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Move columns and write back to excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_move = ['Missing_Species', 'Missing_Diameter', 'Missing_Height','Missing_TAT_Result','Missing_WV_Status']\n",
    "PI_Error_Report = PI_Error_Report[ cols_to_move + [ col for col in PI_Error_Report.columns if col not in cols_to_move ]]\n",
    "PI_Error_Report.to_excel(pi_error_report_xlsx_path,sheet_name=Sheet_Name, index = False)\n",
    "PI_Error_Report = pd.read_excel(pi_error_report_xlsx_path)\n",
    "PI_Error_Report.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate run duration and write to log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = datetime.now().time()\n",
    "\n",
    "t1 = timedelta(hours=start_time.hour, minutes=start_time.minute, seconds=start_time.second, microseconds=start_time.microsecond)\n",
    "t2 = timedelta(hours=end_time.hour, minutes=end_time.minute, seconds=end_time.second, microseconds=end_time.microsecond)\n",
    "\n",
    "time_elapsed = t2 - t1\n",
    "\n",
    "hour = str(time_elapsed).split(\":\")[0]\n",
    "mins = str(time_elapsed).split(\":\")[1]\n",
    "sec = str(time_elapsed).split(\":\")[2].split(\".\")[0]\n",
    "ms = str(time_elapsed).split(\":\")[2].split(\".\")[1]\n",
    "hhmmss = str(time_elapsed).split(\":\")[0] +\":\"+ str(time_elapsed).split(\":\")[1] + \":\" + str(time_elapsed).split(\":\")[2].split(\".\")[0]\n",
    "hhmmssms = str(time_elapsed).split(\":\")[0] +\":\"+ str(time_elapsed).split(\":\")[1] + \":\" + str(time_elapsed).split(\":\")[2].split(\".\")[0]+\".\"+str(time_elapsed).split(\":\")[2].split(\".\")[1]\n",
    "\n",
    "time_log = home_dir + \"\\\\\" + \"Logs\" + \"\\\\\" + \"Time_Logs.csv\"\n",
    "df = pd.DataFrame({'Date_Ran' : [date_time_format],'Time_Elapsed' : [time_elapsed],'HH:MM:SS' : [hhmmss],'HH:MM:SS.MS' : [hhmmssms],'Hours' : [hour],'Min' : [mins],'Sec' : [sec],'Ms' : [ms]})\n",
    "if not os.path.exists(time_log):\n",
    "    df.to_csv(time_log, mode='a',index=False)\n",
    "else:\n",
    "    df.to_csv(time_log, mode='a',index=False,header=False)\n",
    "    \n",
    "df = pd.read_csv(time_log)\n",
    "print(BOLD + BLUE + \"Start Time: \" + NORMAL + str(t1))\n",
    "print(BOLD + BLUE + \"End Time: \" + NORMAL + str(t2))\n",
    "print(BOLD + BLUE + \"Time Elapsed: \" + NORMAL + str(time_elapsed))\n",
    "df.tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit",
   "language": "python",
   "name": "python37664bitec1028d54bbb42a5b11b1a4b50886740"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}